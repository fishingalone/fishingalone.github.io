<html>
    <head> 
        <title>Reinforcement Learning</title>
	<script type="text/javascript" async
  	   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<style>
		body {
			counter-reset: section;
			margin: 5em 20em;
		}
		img {
			display: block;
			margin: 0 auto;
		}
		table {
			margin: 0 auto;
			width: 60%;
			border-collapse: collapse;
		}
		tr:first-child {
			border-top: 1px solid black;
			border-bottom: 1px solid black;
		}
		tr:last-child {
			border-bottom: 1px solid black;
		}
		td {
			text-align: center;
		}
		table.columnsep td {
			border-left: 1px solid black;
			border-right: 1px solid black;
			border-top: 1px solid black;
			border-bottom: 1px solid black;
		}
		h2:before {
			counter-increment: section;
			content: counter(section) ". ";
		}
		p {
			font-size: 14pt;
			line-height: 1.45;
		} 
	</style>
    </head>
    <body>
	<h1>Reinforcement Learning</h1>
	<section>
	<h2>Monte Carlo</h2>
		<h3>Recap</h3>
		<p>In the DP approach, we compute the optimal Q values 
\[
Q^* (s,a) = \sum_{s'} P_{s,s'}^a \left( R_{s,s'}^a + \gamma \max_{a'} Q^*(s',a') \right),
\]
		and derive the optimal policy by choosing
\[
\pi^* = \mathop{\mathrm{arg\, max}}_a Q^*(s,a).
\]
		In reality however, we don't usually have complete information about the environment. The transition probability P and reward R might take too much effort to track down. Instead, we adopt the model-free approach, which gathers information from interacting with the environment.
		</p>
		<h3>Monte Carlo with Exploring Starts</h3>
		<p>Remember that Q is the expected reward, which can be estimated by averaging the reward from multiple runs of the MDP (episodes) in the spirit of Monte Carlo methods. We start with an arbitrary yet deterministic initial policy \( \pi^0\) and do policy evaluation and greedy improvement in each iteration. In the i-th iteration, an episode is generated by following policy \( \pi^i\),
\[
s_0,a_0,r_0,s_1,a_1,r_1,\ldots ,s_n,a_n,r_n.
\]
		We then find the first occurrence of \( s,a\) in the episode \( t = \inf \{ \tau , s_\tau = s, a_\tau = a \} \) and compute the reward \( R = \sum_{k=0}^{n-t} \gamma^k r_{t+k} \). The estimate \( Q^i\) is the average reward from all the episodes before. We improve policy \( \pi^i\) by setting
\[
\pi^{i+1}(s) = \mathop{\mathrm{arg\, max}}_a Q^i (s,a).
\]
		We move onto the (i+1)-th iteration and repeat the whole thing. For the Monte Carlo method to work, all the state-action pairs need to be visited an infinite number of times. If some state-action pair is never visited, then the Monte-Carlo estimate will never improve with experience. One way to solve this issue of maintaining exploration is specify the first step of each episode \( s_0, a_0\) so that every such pair has non zero probability of being selected as the start. We call it the Monte Carlo method with exploring starts.
		</p>
		<h3>On policy Monte Carlo</h3>
		<p>There is an alternative way to maintain exploration other than the technique of exploring starts. We keep the policy <i>soft</i>, which means \( \pi (s,a) > 0 \) for all state-action pairs. We carry out Monte Carlo estimates just as in the MC with ES. The only difference lies in the policy update step. Instead of always choosing the best action, we explore other options with probability \( \epsilon \). Specifically, the current best action is
\[
a^* = \mathop{\mathrm{arg\, max}}_a Q^i (s,a).
\]
		We update the policy by setting for all \( a \in A(s) \),
\[
\pi^{i+1} (s,a) = \begin{cases} 1-\epsilon + \epsilon / |A(s)|, & \text{ if } a = a^*, \\
\epsilon / | A(s)|, & \text{ if } a \neq a^*, \end{cases}.
\]
		</p>
		<h3>Off policy Monte Carlo</h3>
		<p>In the off policy MC method, we adopt another strategy to maintain exploration. We explore with an auxiliary fixed soft policy \( \pi' \) (called behavior policy) and make improvements on the desired deterministic policy \( \pi\) (called estimation policy). We do greedy improvement just as in the MC with ES. The problem is how to estimate Q values for policy \( \pi\) from the episodes generated following policy \( \pi'\).
		The solution is a well known technique called importance sampling. Let's say we generated the i-th (i=1,2,\ldots ,N) episode following \( \pi'\),
\[
s_0^i,a_0^i,r_0^i,\ldots ,s_{n_i}^i,a_{n_i}^i,r_{n_i}^i.
\]
		As in MCES, we find the first occurrence \( t\) of \( (s,a)\) and compute the reward \( R_i(s,a)\). The likelihood of receiving such reward is
\[
p_i'(s,a) = \prod_{k=t}^n \pi' (s_k^i,a_k^i) P_{s_k^i,s_{k+1}^i}^{a_k^i}.
\]
		If we follow policy \( \pi\), the likelihood is instead
\[
p_i (s,a) = \prod_{k=t}^{n_i} \pi (s_k^i,a_k^i) P_{s_k^i,s_{k+1}^i}^{a_k^i}.
\]
		Finally, we can estimate the Q value as
\[
Q(s,a) \approx {\sum_{i=1}^N {p_i(s,a)\over p_i'(s,a)}R_i(s,a)\over \sum_{i=1}^N {p_i(s,a)\over p_i'(s,a)}},
\]
		where the relative probability is independent of environmental dynamics
\[
{p_i(s,a)\over p_i'(s,a)} = \prod_{k=t}^{n_i} {\pi (s_k,a_k)\over \pi '(s_k,a_k)}.
\]
		</p>
	</section>
    </body> 
</html>
